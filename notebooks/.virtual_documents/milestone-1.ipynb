import re
import os
import sys
import glob
import zipfile
import requests
from urllib.request import urlretrieve
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
import timeit


article_id = 14096681
url = f"https://api.figshare.com/v2/articles/{article_id}"
headers = {"Content-Type": "application/json"}
output_directory = "..\data"
file_to_download = "data.zip"
rerun = True


start = timeit.default_timer()
response = requests.request("GET", url, headers=headers)
data = json.loads(response.text)
files = data["files"]
print(f"Run time: {str(datetime.timedelta(minutes=timeit.default_timer()-start))}.")
files


start = timeit.default_timer()
if os.path.exists(f"{output_directory}\\{file_to_download}"):
    print("data.zip is already exists!")
else:
    files_to_dl = ["data.zip"]

    for file in files:
        if file["name"] in files_to_dl:
            os.makedirs(output_directory, exist_ok=True)
            urlretrieve(file["download_url"], os.path.join(output_directory, file["name"]))
print(f"Run time: {str(datetime.timedelta(minutes=int(timeit.default_timer()-start)))}.")


start = timeit.default_timer()
if rerun:
    with zipfile.ZipFile(os.path.join(output_directory, file_to_download), 'r') as f:
        f.extractall(output_directory)
else:
    print("Some CSV files already exists, nothing is extraced. Please check if files in data directory are correct.")
print(f"Run time: {str(datetime.timedelta(minutes=int(timeit.default_timer()-start)))}.")


start = timeit.default_timer()
if rerun:
    files = glob.glob(f'{output_directory}/*.csv')
    files = [f for f in files if f.find("observed_daily_rainfall_SYD.csv")==-1 and f.find("combined_data.csv")==-1]
    
    if len(files) != 27:
        print("Expected 27 files.")
        
    i = 1
    records = 0

    for file in files:
        df = pd.read_csv(file, index_col=0, parse_dates=True).assign(model=re.findall(r'[^\/&\\]+(?=_daily_rainfall_NSW\.)', file)[0])
        print(f"Processing {file:<60} total {len(df)}{' rows, ':<25}{i} out of {len(files)} files.")
        records += len(df)

        if i == 1:
            df.to_csv(f"{output_directory}/combined_data.csv")
        else:
            df.to_csv(f'{output_directory}/combined_data.csv', mode='a', header=False)

        i+=1
    print("")
    print(f"Total rows: {records}.") #62467843 rows
    print("")
print(f"Run time: {str(datetime.timedelta(minutes=int(timeit.default_timer()-start)))}.")


get_ipython().run_cell_magic("sh", "", """du -sh "../data/combined_data.csv"""")


start = timeit.default_timer()
df = pd.read_csv(f"{output_directory}/combined_data.csv", index_col=0, parse_dates=True)
print("Output:")
print("")
print(df["model"].value_counts())
print("-"*50)
print(f"Run time: {str(datetime.timedelta(minutes=int(timeit.default_timer()-start)))}.")
print(f"The size of df in memory: {round(sys.getsizeof(df) / (1024 * 1024 * 1024), 2)} GB.")


start = timeit.default_timer()
dtypes = {'lat_min': 'float16', 
          'lat_max': 'float16', 
          'lon_min': 'float16', 
          'lon_max': 'float16',
          'rain (mm/day)':'float32',
          'model':'str'}
df = pd.read_csv(f"{output_directory}/combined_data.csv",index_col=0, parse_dates=True, dtype=dtypes)
print("Output:")
print("")
print(df["model"].value_counts())
print("-"*50)
print(f"Run time: {str(datetime.timedelta(minutes=int(timeit.default_timer()-start)))}.")
print(f"The size of df in memory: {round(sys.getsizeof(df) / (1024 * 1024 * 1024), 2)} GB.")
print("")
print(f"Column types:")
df.dtypes


start = timeit.default_timer()
use_cols = ["time","rain (mm/day)","model"]
df = pd.read_csv(f"{output_directory}/combined_data.csv",index_col=0, parse_dates=True, usecols=use_cols)
print("Output:")
print("")
print(df["model"].value_counts())
print("-"*50)
print(f"Run time: {str(datetime.timedelta(minutes=int(timeit.default_timer()-start)))}.")
print(f"The size of df in memory: {round(sys.getsizeof(df) / (1024 * 1024 * 1024), 2)} GB.")


start = timeit.default_timer()
use_cols = ["time","rain (mm/day)","model"]
dtypes = {'rain (mm/day)':'float32',
          'model':'str'}
df = pd.read_csv(f"{output_directory}/combined_data.csv",index_col=0, parse_dates=True, usecols=use_cols,dtype=dtypes)
print("Output:")
print("")
print(df["model"].value_counts())
print("-"*50)
print(f"Run time: {str(datetime.timedelta(minutes=int(timeit.default_timer()-start)))}.")
print(f"The size of df in memory: {round(sys.getsizeof(df) / (1024 * 1024 * 1024), 2)} GB.")


start = timeit.default_timer()
counts = pd.Series(dtype=int)

df = pd.read_csv(f"{output_directory}/combined_data.csv", chunksize=1_000_000 )

for chunk in pd.read_csv(f"{output_directory}/combined_data.csv", chunksize=1_000_000):
    counts = counts.add(chunk["model"].value_counts(), fill_value=0)

print("Output:")
print("")
print(counts.astype(int))
print("-"*50)
print(f"Run time: {str(datetime.timedelta(minutes=int(timeit.default_timer()-start)))}.")


if rerun:
    df = pd.read_csv(f"{output_directory}/combined_data.csv",index_col=0, parse_dates=True)


start = timeit.default_timer()
if rerun:
    df.to_parquet(f"{output_directory}/combined_data.parquet")
print(f"Run time: {str(datetime.timedelta(minutes=int(timeit.default_timer()-start)))}.")


start = timeit.default_timer()
if rerun:
    df.to_parquet(f"{output_directory}/combined_data_partition.parquet",partition_cols=['model'])
print(f"Run time: {str(datetime.timedelta(minutes=int(timeit.default_timer()-start)))}.")


get_ipython().run_cell_magic("sh", "", """du -sh "../data/combined_data.csv"
du -sh "../data/combined_data.parquet"
du -sh "../data/combined_data_partition.parquet"""")


start = timeit.default_timer()
df = pd.read_parquet(f"{output_directory}/combined_data.parquet")
print("Output:")
print("")
print(df["model"].value_counts())
print("-"*50)
print(f"Run time: {str(datetime.timedelta(minutes=int(timeit.default_timer()-start)))}.")
print(f"The size of df in memory: {round(sys.getsizeof(df) / (1024 * 1024 * 1024), 2)} GB.")


get_ipython().run_line_magic("load_ext", " rpy2.ipython")


rpy2.ipython.rmagic.RMagics.R("""
suppressPackageStartupMessages(library(dplyr,quietly=TRUE))

start <- Sys.time()
output_directory <- "../data"
#df <- arrow::open_dataset(paste0(output_directory,"/","combined_data.parquet"))
df <- arrow::open_dataset(paste0(output_directory,"/","combined_data_partition.parquet"), 
                          format="parquet", 
                          partitioning=c("model"))
df %>%
    group_by(model) %>%
    summarize(cnt=n()) %>%
    ungroup() %>%
    collect()

Sys.time() - start""", "")
