import re
import os
import sys
import glob
import zipfile
import requests
from urllib.request import urlretrieve
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots


article_id = 14096681
url = f"https://api.figshare.com/v2/articles/{article_id}"
headers = {"Content-Type": "application/json"}
output_directory = "../data/"
file_to_download = "data.zip"
rerun = True


get_ipython().run_cell_magic("time", "", """response = requests.request("GET", url, headers=headers)
data = json.loads(response.text)
files = data["files"]
files""")


get_ipython().run_cell_magic("time", "", """if os.path.exists(f"{output_directory}/{file_to_download}"):
    print("data.zip is already exists!")
else:
    files_to_dl = ["data.zip"]

    for file in files:
        if file["name"] in files_to_dl:
            os.makedirs(output_directory, exist_ok=True)
            urlretrieve(file["download_url"], output_directory + file["name"])""")


get_ipython().run_cell_magic("time", "", """if rerun:
    with zipfile.ZipFile(os.path.join(output_directory, file_to_download), 'r') as f:
        f.extractall(output_directory)
else:
    print("Some CSV files already exists, nothing is extraced. Please check if files in data directory are correct.")""")


#%%time
#files = glob.glob(f'{output_directory}/*.csv')
#files = [f for f in files if f.find("observed_daily_rainfall_SYD.csv")==-1 and f.find("combined_data.csv")==-1]

#df = pd.concat((pd.read_csv(file, index_col=0, parse_dates=True).assign(model=re.findall(r'(?<=\\)(.*)(?=_daily)', file)[0])
#                for file in files))
#print(df.shape)


get_ipython().run_cell_magic("time", "", """
if rerun:
    files = glob.glob(f'{output_directory}/*.csv')
    files = [f for f in files if f.find("observed_daily_rainfall_SYD.csv")==-1 and f.find("combined_data.csv")==-1]
    i = 1
    records = 0

    for file in files:
        df = pd.read_csv(file, index_col=0, parse_dates=True).assign(model=re.findall(r'(?<=\\)(.*)(?=_daily)', file)[0])
        print(f"Processing {file} \t total {len(df)} rows, \t {i} out of {len(files)} files.")
        records += len(df)

        if i == 1:
            df.to_csv(f"{output_directory}/combined_data.csv")
        else:
            df.to_csv(f'{output_directory}/combined_data.csv', mode='a', header=False)

        i+=1
    print("")
    print(f"Total rows: {records}.") #62467843 rows
    print("")""")


get_ipython().run_cell_magic("sh", "", """du -sh "../data/combined_data.csv"""")


get_ipython().run_cell_magic("time", "", """
df = pd.read_csv(f"{output_directory}/combined_data.csv",index_col=0, parse_dates=True)
print("")
print(df.info(memory_usage='deep'))
print("")
print(df["model"].value_counts())""")


get_ipython().run_cell_magic("time", "", """
dtypes = {'lat_min': 'float16', 
          'lat_max': 'float16', 
          'lon_min': 'float16', 
          'lon_max': 'float16',
          'rain (mm/day)':'float32',
          'model':'str'}
df = pd.read_csv(f"{output_directory}/combined_data.csv",index_col=0, parse_dates=True, dtype=dtypes)
print("")
print(df.info(memory_usage='deep'))
print("")
print(df["model"].value_counts())""")


get_ipython().run_cell_magic("time", "", """
use_cols = ["time","rain (mm/day)","model"]
df = pd.read_csv(f"{output_directory}/combined_data.csv",index_col=0, parse_dates=True, usecols=use_cols)
print("")
print(df.info(memory_usage='deep'))
print("")
print(df["model"].value_counts())""")


get_ipython().run_cell_magic("time", "", """
use_cols = ["time","rain (mm/day)","model"]
dtypes = {'rain (mm/day)':'float32',
          'model':'str'}
df = pd.read_csv(f"{output_directory}/combined_data.csv",index_col=0, parse_dates=True, usecols=use_cols,dtype=dtypes)
print("")
print(df.info(memory_usage='deep'))
print("")
print(df["model"].value_counts())""")


get_ipython().run_cell_magic("time", "", """counts = pd.Series(dtype=int)
for chunk in pd.read_csv(f"{output_directory}/combined_data.csv", chunksize=1_000_000):
    counts = counts.add(chunk["model"].value_counts(), fill_value=0)
print(counts.astype(int))""")
